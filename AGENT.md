````markdown
# üß† RL Agent Design for Ti·∫øn L√™n

## Neural Network Architecture

```python
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, action_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)
```
````

## üîÑ Quy Tr√¨nh H·ªçc T·∫≠p

1. **Experience Replay**

   - L∆∞u tr·ªØ (state, action, reward, next_state) v√†o replay buffer
   - L·∫•y m·∫´u ng·∫´u nhi√™n khi hu·∫•n luy·ªán

2. **Target Network**

   - S·ª≠ d·ª•ng 2 m·∫°ng ri√™ng bi·ªát: policy_net v√† target_net
   - C·∫≠p nh·∫≠t target_net sau m·ªói 1000 b∆∞·ªõc

3. **Loss Function**

```python
loss = F.smooth_l1_loss(
    Q_values,
    expected_Q_values.unsqueeze(1)
)
```

## üéÆ C∆° Ch·∫ø Ch∆°i Game

```mermaid
graph TD
    A[Receive 13 cards] --> B{My turn?}
    B -->|Yes| C[Encode state]
    C --> D[Predict Q-values]
    D --> E[Select valid action]
    E --> F[Execute action]
    F --> G[Receive reward]
    B -->|No| H[Observe opponents]
```

## ‚öôÔ∏è Hyperparameters

| Parameter       | Value   |
| --------------- | ------- |
| Learning Rate   | 0.00025 |
| Discount Factor | 0.95    |
| Epsilon Decay   | 0.9995  |
| Batch Size      | 64      |

```

```
