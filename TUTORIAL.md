# üéì Building Ti·∫øn L√™n AI: Step-by-Step

## 1. Bi·ªÉu di·ªÖn tr·∫°ng th√°i (State Representation)

```python
def get_state(self):
    """Tensor 6x4x13 bi·ªÉu di·ªÖn:
    0: B√†i tr√™n tay
    4: Combo hi·ªán t·∫°i
    5: L·ªãch s·ª≠ n∆∞·ªõc ƒëi"""
    state = np.zeros((6, 4, 13), dtype=np.float32)
    # ... m√£ h√≥a b√†i ...
    return state
```

## 2. X√°c ƒë·ªãnh h√†nh ƒë·ªông h·ª£p l·ªá

```python
def get_valid_combos(self, player_idx):
    valid_combos = []
    # PASS lu√¥n h·ª£p l·ªá
    valid_combos.append(("PASS", []))
    # Singles
    valid_combos.extend([("SINGLE", [card]) for card in player.hand])
    # Pairs
    for rank, cards in rank_counts.items():
        if len(cards) >= 2:
            valid_combos.append(("PAIR", cards[:2]))
    # ... c√°c combo kh√°c ...
    return valid_combos
```

## 3. Ki·∫øn tr√∫c m·∫°ng Neural

```python
class TienLenNet(nn.Module):
    def __init__(self):
        super().__init__()
        # Convolutional backbone
        self.conv1 = nn.Conv2d(6, 128, kernel_size=3, padding=1)
        self.res_blocks = nn.Sequential(...)

        # Policy head
        self.policy_conv = nn.Conv2d(128, 4, kernel_size=1)
        self.policy_out = nn.Linear(256, 200)

        # Value head
        self.value_conv = nn.Conv2d(128, 2, kernel_size=1)
        self.value_out = nn.Linear(128, 1)
```

## 4. MCTS v·ªõi Continual Resolving

```python
class MCTS:
    def run(self, game):
        root = Node(game.copy(), model=self.model)
        for _ in range(self.num_simulations):
            node = root
            # Selection
            while not node.is_leaf():
                node = node.select_child()
            # Expansion
            if not node.resolved:
                node.resolve()
            # Simulation
            value = node.resolve() if not node.game_instance.done else reward
            # Backpropagation
            node.update(value)
```

## 5. Hu·∫•n luy·ªán v·ªõi PPO

```python
def ppo_train(model, buffer, optimizer, scaler, clip_epsilon=0.2, ...):
    # Compute advantages
    advantages = buffer.compute_advantages()

    # PPO updates
    for _ in range(ppo_epochs):
        # Policy loss
        ratio = (new_log_probs - old_log_probs).exp()
        policy_loss = -torch.min(ratio * advantages,
                                torch.clamp(ratio, 1-clip_epsilon, 1+clip_epsilon) * advantages).mean()

        # Update model
        optimizer.zero_grad()
        scaler.scale(loss).backward()
        scaler.step(optimizer)
```

## 6. T·ªëi ∆∞u hi·ªáu nƒÉng

- **State Caching**: L∆∞u tr·ªØ valid actions
- **Vectorization**: M√£ h√≥a b√†i d·∫°ng tensor
- **Compression**: N√©n d·ªØ li·ªáu replay v·ªõi LZ4

```python
class ReplayBuffer:
    def push(self, trajectory):
        compressed = lz4.frame.compress(pickle.dumps(trajectory))
        self.buffer.append(compressed)
```

## 7. Ch·∫°y hu·∫•n luy·ªán

```python
if __name__ == "__main__":
    # GPU setup
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Initialize model
    model = TienLenNet().to(device)

    # Start training
    main_train_loop()
```
